{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2019-06-06 15:17:59--  https://raw.githubusercontent.com/opencv/opencv/master/data/haarcascades/haarcascade_frontalface_default.xml\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.112.133\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.112.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 930127 (908K) [text/plain]\n",
      "Saving to: ‘haarcascade_frontalface_default.xml’\n",
      "\n",
      "haarcascade_frontal 100%[===================>] 908.33K  1.98MB/s    in 0.4s    \n",
      "\n",
      "2019-06-06 15:18:00 (1.98 MB/s) - ‘haarcascade_frontalface_default.xml’ saved [930127/930127]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/opencv/opencv/master/data/haarcascades/haarcascade_frontalface_default.xml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2019-06-06 16:00:47--  https://raw.githubusercontent.com/opencv/opencv/master/data/haarcascades/haarcascade_frontalcatface_extended.xml\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.12.133\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.12.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 382918 (374K) [text/plain]\n",
      "Saving to: ‘haarcascade_frontalcatface_extended.xml’\n",
      "\n",
      "haarcascade_frontal 100%[===================>] 373.94K  2.16MB/s    in 0.2s    \n",
      "\n",
      "2019-06-06 16:00:48 (2.16 MB/s) - ‘haarcascade_frontalcatface_extended.xml’ saved [382918/382918]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/opencv/opencv/master/data/haarcascades/haarcascade_frontalcatface_extended.xml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting imutils\n",
      "  Downloading https://files.pythonhosted.org/packages/5e/0c/659c2bdae8e8ca5ef810b9da02db28feaa29ea448ff36b65a1664ff28142/imutils-0.5.2.tar.gz\n",
      "Building wheels for collected packages: imutils\n",
      "  Building wheel for imutils (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /Users/shamanu4/Library/Caches/pip/wheels/b2/40/59/139d450e68847ef2f27d876d527b13389dac23df0f66526b5d\n",
      "Successfully built imutils\n",
      "Installing collected packages: imutils\n",
      "Successfully installed imutils-0.5.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install imutils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2015-2016 Carnegie Mellon University\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "\"\"\"Module for dlib-based alignment.\"\"\"\n",
    "\n",
    "import cv2\n",
    "import dlib\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "TEMPLATE = np.float32([\n",
    "    (0.0792396913815, 0.339223741112), (0.0829219487236, 0.456955367943),\n",
    "    (0.0967927109165, 0.575648016728), (0.122141515615, 0.691921601066),\n",
    "    (0.168687863544, 0.800341263616), (0.239789390707, 0.895732504778),\n",
    "    (0.325662452515, 0.977068762493), (0.422318282013, 1.04329000149),\n",
    "    (0.531777802068, 1.06080371126), (0.641296298053, 1.03981924107),\n",
    "    (0.738105872266, 0.972268833998), (0.824444363295, 0.889624082279),\n",
    "    (0.894792677532, 0.792494155836), (0.939395486253, 0.681546643421),\n",
    "    (0.96111933829, 0.562238253072), (0.970579841181, 0.441758925744),\n",
    "    (0.971193274221, 0.322118743967), (0.163846223133, 0.249151738053),\n",
    "    (0.21780354657, 0.204255863861), (0.291299351124, 0.192367318323),\n",
    "    (0.367460241458, 0.203582210627), (0.4392945113, 0.233135599851),\n",
    "    (0.586445962425, 0.228141644834), (0.660152671635, 0.195923841854),\n",
    "    (0.737466449096, 0.182360984545), (0.813236546239, 0.192828009114),\n",
    "    (0.8707571886, 0.235293377042), (0.51534533827, 0.31863546193),\n",
    "    (0.516221448289, 0.396200446263), (0.517118861835, 0.473797687758),\n",
    "    (0.51816430343, 0.553157797772), (0.433701156035, 0.604054457668),\n",
    "    (0.475501237769, 0.62076344024), (0.520712933176, 0.634268222208),\n",
    "    (0.565874114041, 0.618796581487), (0.607054002672, 0.60157671656),\n",
    "    (0.252418718401, 0.331052263829), (0.298663015648, 0.302646354002),\n",
    "    (0.355749724218, 0.303020650651), (0.403718978315, 0.33867711083),\n",
    "    (0.352507175597, 0.349987615384), (0.296791759886, 0.350478978225),\n",
    "    (0.631326076346, 0.334136672344), (0.679073381078, 0.29645404267),\n",
    "    (0.73597236153, 0.294721285802), (0.782865376271, 0.321305281656),\n",
    "    (0.740312274764, 0.341849376713), (0.68499850091, 0.343734332172),\n",
    "    (0.353167761422, 0.746189164237), (0.414587777921, 0.719053835073),\n",
    "    (0.477677654595, 0.706835892494), (0.522732900812, 0.717092275768),\n",
    "    (0.569832064287, 0.705414478982), (0.635195811927, 0.71565572516),\n",
    "    (0.69951672331, 0.739419187253), (0.639447159575, 0.805236879972),\n",
    "    (0.576410514055, 0.835436670169), (0.525398405766, 0.841706377792),\n",
    "    (0.47641545769, 0.837505914975), (0.41379548902, 0.810045601727),\n",
    "    (0.380084785646, 0.749979603086), (0.477955996282, 0.74513234612),\n",
    "    (0.523389793327, 0.748924302636), (0.571057789237, 0.74332894691),\n",
    "    (0.672409137852, 0.744177032192), (0.572539621444, 0.776609286626),\n",
    "    (0.5240106503, 0.783370783245), (0.477561227414, 0.778476346951)])\n",
    "\n",
    "TPL_MIN, TPL_MAX = np.min(TEMPLATE, axis=0), np.max(TEMPLATE, axis=0)\n",
    "MINMAX_TEMPLATE = (TEMPLATE - TPL_MIN) / (TPL_MAX - TPL_MIN)\n",
    "\n",
    "\n",
    "class AlignDlib:\n",
    "    \"\"\"\n",
    "    Use `dlib's landmark estimation <http://blog.dlib.net/2014/08/real-time-face-pose-estimation.html>`_ to align faces.\n",
    "\n",
    "    The alignment preprocess faces for input into a neural network.\n",
    "    Faces are resized to the same size (such as 96x96) and transformed\n",
    "    to make landmarks (such as the eyes and nose) appear at the same\n",
    "    location on every image.\n",
    "\n",
    "    Normalized landmarks:\n",
    "\n",
    "    .. image:: ../images/dlib-landmark-mean.png\n",
    "    \"\"\"\n",
    "\n",
    "    #: Landmark indices.\n",
    "    INNER_EYES_AND_BOTTOM_LIP = [39, 42, 57]\n",
    "    OUTER_EYES_AND_NOSE = [36, 45, 33]\n",
    "\n",
    "    def __init__(self, facePredictor):\n",
    "        \"\"\"\n",
    "        Instantiate an 'AlignDlib' object.\n",
    "\n",
    "        :param facePredictor: The path to dlib's\n",
    "        :type facePredictor: str\n",
    "        \"\"\"\n",
    "        assert facePredictor is not None\n",
    "\n",
    "        self.detector = dlib.get_frontal_face_detector()\n",
    "        self.predictor = dlib.shape_predictor(facePredictor)\n",
    "\n",
    "    def getAllFaceBoundingBoxes(self, rgbImg):\n",
    "        \"\"\"\n",
    "        Find all face bounding boxes in an image.\n",
    "\n",
    "        :param rgbImg: RGB image to process. Shape: (height, width, 3)\n",
    "        :type rgbImg: numpy.ndarray\n",
    "        :return: All face bounding boxes in an image.\n",
    "        :rtype: dlib.rectangles\n",
    "        \"\"\"\n",
    "        assert rgbImg is not None\n",
    "\n",
    "        try:\n",
    "            return self.detector(rgbImg, 1)\n",
    "        except Exception as e:\n",
    "            print(\"Warning: {}\".format(e))\n",
    "            # In rare cases, exceptions are thrown.\n",
    "            return []\n",
    "\n",
    "    def getLargestFaceBoundingBox(self, rgbImg, skipMulti=False):\n",
    "        \"\"\"\n",
    "        Find the largest face bounding box in an image.\n",
    "\n",
    "        :param rgbImg: RGB image to process. Shape: (height, width, 3)\n",
    "        :type rgbImg: numpy.ndarray\n",
    "        :param skipMulti: Skip image if more than one face detected.\n",
    "        :type skipMulti: bool\n",
    "        :return: The largest face bounding box in an image, or None.\n",
    "        :rtype: dlib.rectangle\n",
    "        \"\"\"\n",
    "        assert rgbImg is not None\n",
    "\n",
    "        faces = self.getAllFaceBoundingBoxes(rgbImg)\n",
    "        if (not skipMulti and len(faces) > 0) or len(faces) == 1:\n",
    "            return max(faces, key=lambda rect: rect.width() * rect.height())\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    def findLandmarks(self, rgbImg, bb):\n",
    "        \"\"\"\n",
    "        Find the landmarks of a face.\n",
    "\n",
    "        :param rgbImg: RGB image to process. Shape: (height, width, 3)\n",
    "        :type rgbImg: numpy.ndarray\n",
    "        :param bb: Bounding box around the face to find landmarks for.\n",
    "        :type bb: dlib.rectangle\n",
    "        :return: Detected landmark locations.\n",
    "        :rtype: list of (x,y) tuples\n",
    "        \"\"\"\n",
    "        assert rgbImg is not None\n",
    "        assert bb is not None\n",
    "\n",
    "        points = self.predictor(rgbImg, bb)\n",
    "        return list(map(lambda p: (p.x, p.y), points.parts()))\n",
    "\n",
    "    def align(self, imgDim, rgbImg, bb=None,\n",
    "              landmarks=None, landmarkIndices=INNER_EYES_AND_BOTTOM_LIP,\n",
    "              skipMulti=False):\n",
    "        r\"\"\"align(imgDim, rgbImg, bb=None, landmarks=None, landmarkIndices=INNER_EYES_AND_BOTTOM_LIP)\n",
    "\n",
    "        Transform and align a face in an image.\n",
    "\n",
    "        :param imgDim: The edge length in pixels of the square the image is resized to.\n",
    "        :type imgDim: int\n",
    "        :param rgbImg: RGB image to process. Shape: (height, width, 3)\n",
    "        :type rgbImg: numpy.ndarray\n",
    "        :param bb: Bounding box around the face to align. \\\n",
    "                   Defaults to the largest face.\n",
    "        :type bb: dlib.rectangle\n",
    "        :param landmarks: Detected landmark locations. \\\n",
    "                          Landmarks found on `bb` if not provided.\n",
    "        :type landmarks: list of (x,y) tuples\n",
    "        :param landmarkIndices: The indices to transform to.\n",
    "        :type landmarkIndices: list of ints\n",
    "        :param skipMulti: Skip image if more than one face detected.\n",
    "        :type skipMulti: bool\n",
    "        :return: The aligned RGB image. Shape: (imgDim, imgDim, 3)\n",
    "        :rtype: numpy.ndarray\n",
    "        \"\"\"\n",
    "        assert imgDim is not None\n",
    "        assert rgbImg is not None\n",
    "        assert landmarkIndices is not None\n",
    "\n",
    "        if bb is None:\n",
    "            bb = self.getLargestFaceBoundingBox(rgbImg, skipMulti)\n",
    "            if bb is None:\n",
    "                return\n",
    "\n",
    "        if landmarks is None:\n",
    "            landmarks = self.findLandmarks(rgbImg, bb)\n",
    "\n",
    "        npLandmarks = np.float32(landmarks)\n",
    "        npLandmarkIndices = np.array(landmarkIndices)\n",
    "\n",
    "        H = cv2.getAffineTransform(npLandmarks[npLandmarkIndices],\n",
    "                                   imgDim * MINMAX_TEMPLATE[npLandmarkIndices])\n",
    "        thumbnail = cv2.warpAffine(rgbImg, H, (imgDim, imgDim))\n",
    "\n",
    "        return thumbnail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime \n",
    "\n",
    "class FPS:\n",
    "    def __init__(self):\n",
    "        # store the start time, end time, and total number of frames\n",
    "        # that were examined between the start and end intervals\n",
    "        self._start = None\n",
    "        self._end = None\n",
    "        self._numFrames = 0\n",
    "\n",
    "    def start(self):\n",
    "        # start the timer\n",
    "        self._start = datetime.datetime.now()\n",
    "        return self\n",
    "\n",
    "    def stop(self):\n",
    "        # stop the timer\n",
    "        self._end = datetime.datetime.now()\n",
    "\n",
    "    def update(self):\n",
    "        # increment the total number of frames examined during the\n",
    "        # start and end intervals\n",
    "        self._numFrames += 1\n",
    "        self._end = datetime.datetime.now()\n",
    "\n",
    "    def elapsed(self):\n",
    "        # return the total number of seconds between the start and\n",
    "        # end interval\n",
    "        return (self._end - self._start).total_seconds()\n",
    "\n",
    "    def fps(self):\n",
    "        # compute the (approximate) frames per second\n",
    "        return self._numFrames / self.elapsed()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "## import cv2\n",
    "import dlib\n",
    "from imutils.video import FileVideoStream\n",
    "\n",
    "\n",
    "cascPath = \"haarcascade_frontalface_default.xml\"\n",
    "# cascPath = \"haarcascade_frontalcatface_extended.xml\"\n",
    "\n",
    "device = \"https://edge-pl-wroclaw.liveedu.tv:9443/vod/eu/universal/smil:elliottminns_20180402172643.smil/playlist.m3u8?wmsAuthSign=c2VydmVyX3RpbWU9Ni8wNi8yMDE5IDI6MTE6MDQgUE0maGFzaF92YWx1ZT1xMkMwdm8zbzB0STlPdkVMTzRUTzZnPT0mdmFsaWRtaW51dGVzPTE0NDAmaWQ9YzJoaGJXRnVkVFF2YkVzMmNrd3ZTbkExZVVVdlF6TTVOelpCTnpJelFUTkNORVkxUWpoRU56VTNRMEpFUXpRMVJqYzJRVFE9\"\n",
    "device = 0\n",
    "\n",
    "faceCascade = cv2.CascadeClassifier(cascPath)\n",
    "\n",
    "video_capture = cv2.VideoCapture(device)\n",
    "\n",
    "preformat = AlignDlib(\"shape_predictor_68_face_landmarks.dat\")\n",
    "\n",
    "font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "\n",
    "fps = FPS()\n",
    "fps.start()\n",
    "\n",
    "while True:\n",
    "    # Capture frame-by-frame\n",
    "    ret, frame = video_capture.read()\n",
    "    # frame = fvs.read()\n",
    "    fps.update()\n",
    "    \n",
    "    if fps.fps() >= 24:\n",
    "\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        faces, n, weights = faceCascade.detectMultiScale3(\n",
    "            gray,#\n",
    "            scaleFactor=1.2,\n",
    "            outputRejectLevels=True,\n",
    "            minNeighbors=5,\n",
    "            minSize=(100, 100),\n",
    "            flags=cv2.CASCADE_SCALE_IMAGE\n",
    "        )\n",
    "\n",
    "        # Draw a rectangle around the faces\n",
    "        for f, c in zip(faces, weights):\n",
    "            x, y, w, h = f\n",
    "            if c[0] < 3:\n",
    "                continue\n",
    "            # cropped = frame[y:y+h , x:x+w, :]\n",
    "            cropped = preformat.align(300, frame, dlib.rectangle(x,y,x+w,y+h))\n",
    "            try:\n",
    "                fp = fps.fps()\n",
    "            except:\n",
    "                fp = 0\n",
    "\n",
    "            # cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 255, 0), 2)q\n",
    "            # Display the resulting frame\n",
    "            cv2.putText(cropped, \"{0:.2f} {1:.2f}\".format(c[0], fp), (10,20), font, 0.5, (255,255,0), 1, cv2.LINE_AA)\n",
    "            cv2.imshow('Video', cropped)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# When everything is done, release the capture\n",
    "video_capture.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
